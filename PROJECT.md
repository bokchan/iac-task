# Pipeline Orchestration Service - PoC

## Overview
A FastAPI-based REST API service for submitting and tracking Snakemake pipeline jobs.

**Scope**: 6-hour Proof of Concept
**Focus**: Working job submission and tracking with mock pipeline execution
**Infrastructure**: Reuse existing ECS deployment (no CDK changes needed)

## Implementation Plan (6 hours)

### AI-Assisted Development Strategy ü§ñ
**Leverage GitHub Copilot + Claude/ChatGPT throughout for 30-40% faster development:**
- Generate Pydantic models and endpoint boilerplate
- Create test cases and mock data
- Debug issues and suggest fixes
- Generate documentation and examples

**Key AI prompts:**
1. "Generate FastAPI endpoint for job submission with Pydantic models"
2. "Create in-memory storage class with thread-safe operations"
3. "Write pytest test cases for job lifecycle"
4. "Generate OpenAPI documentation examples"

---

### Hour 1-2: FastAPI Application Setup

**1. Job Models & API Endpoints**
```python
# Pydantic models
- JobSubmission (input parameters)
- JobStatus (id, status, created_at, updated_at)
- JobList (paginated response)

# Endpoints
- POST /jobs - Submit new job
- GET /jobs/{job_id} - Get job status
- GET /jobs - List all jobs
```

**2. In-Memory Storage Implementation**
```python
# storage.py - Thread-safe in-memory job storage
import threading
from typing import Dict, List, Optional

class JobStore:
    def __init__(self):
        self._jobs: Dict[str, dict] = {}
        self._lock = threading.Lock()

    def create(self, job_id: str, job_data: dict) -> dict:
        with self._lock:
            self._jobs[job_id] = job_data
            return job_data

    def get(self, job_id: str) -> Optional[dict]:
        with self._lock:
            return self._jobs.get(job_id)

    def update(self, job_id: str, updates: dict) -> Optional[dict]:
        with self._lock:
            if job_id in self._jobs:
                self._jobs[job_id].update(updates)
                return self._jobs[job_id]
            return None

    def list_all(self) -> List[dict]:
        with self._lock:
            return list(self._jobs.values())

# Global instance
job_store = JobStore()
```

**Key Points:**
- ‚úÖ Thread-safe with locks for concurrent access
- ‚úÖ GET endpoints work by looking up jobs in dictionary
- ‚úÖ Real functional storage (not a mock)
- ‚ö†Ô∏è  Data lost on restart (acceptable for PoC)

**3. Mock Pipeline**
- Python function that simulates Snakemake execution
- Configurable delay (10-30 seconds)
- Random success/failure (80% success rate)

---

### Hour 3-4: Background Job Processing

**1. Background Task Worker**
- Use FastAPI `BackgroundTasks` for simplicity
- Process jobs asynchronously after submission
- Update job status during execution

**2. Job Lifecycle**
```
POST /jobs ‚Üí Create job (pending)
             ‚Üì
             Background worker picks up job
             ‚Üì
             Update status (running)
             ‚Üì
             Execute mock pipeline
             ‚Üì
             Update status (completed/failed)
```

**3. Error Handling**
- Try/catch around pipeline execution
- Store error messages in job object

---

### Hour 5-6: Testing & Deployment

**1. Local Testing**
- Test all endpoints with curl/Postman
- Submit multiple concurrent jobs
- Verify status updates

**2. Documentation**
- OpenAPI docs (auto-generated by FastAPI)
- Simple README with usage examples

**3. Deployment**
- Update `webapp/` code
- Push to GitHub
- Existing CI/CD pipeline automatically deploys to ECS
- Test on existing ALB URL

---

## Technical Architecture

### File Structure
```
webapp/
‚îú‚îÄ‚îÄ main.py              # FastAPI app with all endpoints
‚îú‚îÄ‚îÄ models.py            # Pydantic models
‚îú‚îÄ‚îÄ pipeline.py          # Mock pipeline execution
‚îú‚îÄ‚îÄ storage.py           # In-memory job storage
‚îú‚îÄ‚îÄ requirements.txt     # Dependencies (no changes needed)
‚îî‚îÄ‚îÄ Dockerfile           # Existing file (no changes needed)
```

### Technology Choices
- ‚úÖ **Storage**: Thread-safe in-memory dictionary
- ‚úÖ **Queue**: FastAPI BackgroundTasks
- ‚úÖ **Worker**: Same process as API
- ‚úÖ **Pipeline**: Mock Python function
- ‚úÖ **Infrastructure**: Existing ECS (VPC, ECR, OIDC, App Stack)
- ‚úÖ **CI/CD**: Existing GitHub Actions workflow
- ‚úÖ **AI Tools**: GitHub Copilot + Claude/ChatGPT

### Deployment Architecture
**Reuse existing infrastructure:**
- VPC Stack ‚Üí Networking already configured
- ECR Stack ‚Üí Push updated webapp image
- GitHub OIDC Stack ‚Üí Authentication ready
- App Stack ‚Üí ECS Fargate + ALB deployed
- GitHub Actions ‚Üí Automatic build + deploy

**Workflow**: Code ‚Üí Push ‚Üí Auto-deploy (3-5 min) ‚Üí Test on ALB

---

## Implementation Checklist

- [ ] Hour 1: AI-generate Pydantic models and basic endpoint structure
- [ ] Hour 2: Implement thread-safe in-memory storage with AI help
- [ ] Hour 2: Create mock pipeline function
- [ ] Hour 3: Add background task processing with FastAPI BackgroundTasks
- [ ] Hour 3-4: AI-generate test cases and test job lifecycle
- [ ] Hour 4: Test concurrent job submissions (AI-generated test script)
- [ ] Hour 5: AI-generate documentation and usage examples
- [ ] Hour 5: Update README with API examples
- [ ] Hour 6: Deploy to existing ECS via GitHub Actions
- [ ] Hour 6: Verify all endpoints work on deployed ALB

---

## Success Criteria

‚úÖ Can submit a job via POST /jobs
‚úÖ Job status updates from pending ‚Üí running ‚Üí completed
‚úÖ Can retrieve job status via GET /jobs/{id} (real-time from in-memory store)
‚úÖ Can list all jobs via GET /jobs (returns all stored jobs)
‚úÖ Multiple concurrent jobs process correctly (thread-safe storage)
‚úÖ Mock pipeline executes with simulated delay
‚úÖ Deployed to existing ECS infrastructure with zero infrastructure changes
‚úÖ AI-assisted development accelerates implementation by 30-40%

---

## FAQ

**Q: How do GET endpoints work without a database?**
A: Jobs are stored in a Python dictionary with thread-safe locks. When you call GET /jobs/{id}, it looks up the job in memory. Background tasks update the same dictionary. Works perfectly for single-container PoC.

**Q: Is this just mocking the storage?**
A: No! This is real, functional storage - just not persistent. Jobs are actually created, updated, and retrieved. Only difference from a database is data loss on restart.

**Q: Can I test multiple jobs?**
A: Yes! Submit dozens of jobs - they'll all be stored and tracked correctly during application runtime.

**Q: What happens on ECS restart/redeploy?**
A: All job data is lost. For PoC demo, simply submit new test jobs after deployment.

---

## Future Enhancements (Out of Scope)

If you need to extend beyond PoC (+3-4 hours for PostgreSQL):
- Persistent storage with RDS PostgreSQL
- Separate worker service for better scaling
- Authentication/Authorization
- Production monitoring and alarms
- Comprehensive testing suite
- Message queue (SQS) for better job distribution

